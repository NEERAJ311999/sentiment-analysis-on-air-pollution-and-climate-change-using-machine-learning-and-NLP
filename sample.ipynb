{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\myenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in .\\myenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: word2number in .\\myenv\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in .\\myenv\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\myenv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\myenv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: click in .\\myenv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in .\\myenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in .\\myenv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in .\\myenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in .\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in .\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk word2number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from word2number import w2n\n",
    "from nltk import download\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial_number</th>\n",
       "      <th>website_name</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Times of India</td>\n",
       "      <td>https://timesofindia.indiatimes.com/life-style...</td>\n",
       "      <td>GRAP Stage IV restrictions reimposed in Delhi-...</td>\n",
       "      <td>Jan 15, 2025, 20:35 IST</td>\n",
       "      <td>Delhi-NCR is once again grappling with severe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Times of India</td>\n",
       "      <td>https://timesofindia.indiatimes.com/education/...</td>\n",
       "      <td>Delhi schools directed to shift to hybrid mode...</td>\n",
       "      <td>Jan 16, 2025, 08:12 IST</td>\n",
       "      <td>On January 15, the Delhi Directorate of Educat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Times of India</td>\n",
       "      <td>https://timesofindia.indiatimes.com/life-style...</td>\n",
       "      <td>The hidden dangers of air pollution and fog on...</td>\n",
       "      <td>Jan 10, 2025, 17:15 IST</td>\n",
       "      <td>Human exposure to unhealthy levels of air poll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Times of India</td>\n",
       "      <td>https://timesofindia.indiatimes.com/technology...</td>\n",
       "      <td>“India would save more years of life by…”: Tec...</td>\n",
       "      <td>Updated: Jan 7, 2025, 13:34 IST</td>\n",
       "      <td>Image source: X\\nAmerican tech entrepreneur Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Times of India</td>\n",
       "      <td>https://timesofindia.indiatimes.com/city/mumba...</td>\n",
       "      <td>Air pollution alarming at multiple locations</td>\n",
       "      <td>Jan 9, 2025, 22:44 IST</td>\n",
       "      <td>Mumbai: For a major part of November and Decem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   serial_number    website_name  \\\n",
       "0            1.0  Times of India   \n",
       "1            2.0  Times of India   \n",
       "2            3.0  Times of India   \n",
       "3            4.0  Times of India   \n",
       "4            5.0  Times of India   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://timesofindia.indiatimes.com/life-style...   \n",
       "1  https://timesofindia.indiatimes.com/education/...   \n",
       "2  https://timesofindia.indiatimes.com/life-style...   \n",
       "3  https://timesofindia.indiatimes.com/technology...   \n",
       "4  https://timesofindia.indiatimes.com/city/mumba...   \n",
       "\n",
       "                                               title  \\\n",
       "0  GRAP Stage IV restrictions reimposed in Delhi-...   \n",
       "1  Delhi schools directed to shift to hybrid mode...   \n",
       "2  The hidden dangers of air pollution and fog on...   \n",
       "3  “India would save more years of life by…”: Tec...   \n",
       "4       Air pollution alarming at multiple locations   \n",
       "\n",
       "                              date  \\\n",
       "0          Jan 15, 2025, 20:35 IST   \n",
       "1          Jan 16, 2025, 08:12 IST   \n",
       "2          Jan 10, 2025, 17:15 IST   \n",
       "3  Updated: Jan 7, 2025, 13:34 IST   \n",
       "4           Jan 9, 2025, 22:44 IST   \n",
       "\n",
       "                                             content  \n",
       "0  Delhi-NCR is once again grappling with severe ...  \n",
       "1  On January 15, the Delhi Directorate of Educat...  \n",
       "2  Human exposure to unhealthy levels of air poll...  \n",
       "3  Image source: X\\nAmerican tech entrepreneur Br...  \n",
       "4  Mumbai: For a major part of November and Decem...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('sample_articles.csv')\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  GRAP Stage IV restrictions reimposed in DelhiN...   \n",
      "1  Delhi schools directed to shift to hybrid mode...   \n",
      "2  The hidden dangers of air pollution and fog on...   \n",
      "3  India would save more years of life by Tech mi...   \n",
      "4       Air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \n",
      "0  DelhiNCR is once again grappling with severe a...  \n",
      "1  On January 15 the Delhi Directorate of Educati...  \n",
      "2  Human exposure to unhealthy levels of air poll...  \n",
      "3  Image source X American tech entrepreneur Brya...  \n",
      "4  Mumbai For a major part of November and Decemb...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text (remove special characters, extra spaces)\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Check if the value is a string\n",
    "        return ''  # If not, return an empty string\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove special characters (keep only letters, digits, and spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Replace multiple consecutive whitespaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to title and content columns\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Display the first few rows to verify the output\n",
    "print(df[['title', 'content']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF YOU DID NOT RUN OPTIONAL THEN RUN NEXT 4 BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "content    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in title and content columns\n",
    "print(df[['title', 'content']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  GRAP Stage IV restrictions reimposed in DelhiN...   \n",
      "1  Delhi schools directed to shift to hybrid mode...   \n",
      "2  The hidden dangers of air pollution and fog on...   \n",
      "3  India would save more years of life by Tech mi...   \n",
      "4       Air pollution alarming at multiple locations   \n",
      "\n",
      "                                       cleaned_title  \\\n",
      "0  GRAP Stage IV restrictions reimposed in DelhiN...   \n",
      "1  Delhi schools directed to shift to hybrid mode...   \n",
      "2  The hidden dangers of air pollution and fog on...   \n",
      "3  India would save more years of life by Tech mi...   \n",
      "4       Air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \\\n",
      "0  DelhiNCR is once again grappling with severe a...   \n",
      "1  On January 15 the Delhi Directorate of Educati...   \n",
      "2  Human exposure to unhealthy levels of air poll...   \n",
      "3  Image source X American tech entrepreneur Brya...   \n",
      "4  Mumbai For a major part of November and Decemb...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  DelhiNCR is once again grappling with severe a...  \n",
      "1  On January 15 the Delhi Directorate of Educati...  \n",
      "2  Human exposure to unhealthy levels of air poll...  \n",
      "3  Image source X American tech entrepreneur Brya...  \n",
      "4  Mumbai For a major part of November and Decemb...  \n"
     ]
    }
   ],
   "source": [
    "# Apply the cleaning function to title and content\n",
    "df['cleaned_title'] = df['title'].apply(clean_text)\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Show the first few rows to verify the output\n",
    "print(df[['title', 'cleaned_title', 'content', 'cleaned_content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  GRAP Stage IV restrictions reimposed in DelhiN...   \n",
      "1  Delhi schools directed to shift to hybrid mode...   \n",
      "2  The hidden dangers of air pollution and fog on...   \n",
      "3  India would save more years of life by Tech mi...   \n",
      "4       Air pollution alarming at multiple locations   \n",
      "\n",
      "                                       cleaned_title  \\\n",
      "0  GRAP Stage IV restrictions reimposed in DelhiN...   \n",
      "1  Delhi schools directed to shift to hybrid mode...   \n",
      "2  The hidden dangers of air pollution and fog on...   \n",
      "3  India would save more years of life by Tech mi...   \n",
      "4       Air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \\\n",
      "0  DelhiNCR is once again grappling with severe a...   \n",
      "1  On January 15 the Delhi Directorate of Educati...   \n",
      "2  Human exposure to unhealthy levels of air poll...   \n",
      "3  Image source X American tech entrepreneur Brya...   \n",
      "4  Mumbai For a major part of November and Decemb...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  DelhiNCR is once again grappling with severe a...  \n",
      "1  On January 15 the Delhi Directorate of Educati...  \n",
      "2  Human exposure to unhealthy levels of air poll...  \n",
      "3  Image source X American tech entrepreneur Brya...  \n",
      "4  Mumbai For a major part of November and Decemb...  \n"
     ]
    }
   ],
   "source": [
    "# Convert title and content to string type before applying the cleaning function\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['content'] = df['content'].astype(str)\n",
    "\n",
    "# Apply the cleaning function again\n",
    "df['cleaned_title'] = df['title'].apply(clean_text)\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Show the first few rows to verify the output\n",
    "print(df[['title', 'cleaned_title', 'content', 'cleaned_content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  GRAP Stage IV restrictions reimposed in DelhiN...   \n",
      "1  Delhi schools directed to shift to hybrid mode...   \n",
      "2  The hidden dangers of air pollution and fog on...   \n",
      "3  India would save more years of life by Tech mi...   \n",
      "4       Air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \n",
      "0  DelhiNCR is once again grappling with severe a...  \n",
      "1  On January 15 the Delhi Directorate of Educati...  \n",
      "2  Human exposure to unhealthy levels of air poll...  \n",
      "3  Image source X American tech entrepreneur Brya...  \n",
      "4  Mumbai For a major part of November and Decemb...  \n",
      "                                       cleaned_title  \\\n",
      "0  GRAP Stage IV restrictions reimposed in DelhiN...   \n",
      "1  Delhi schools directed to shift to hybrid mode...   \n",
      "2  The hidden dangers of air pollution and fog on...   \n",
      "3  India would save more years of life by Tech mi...   \n",
      "4       Air pollution alarming at multiple locations   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  DelhiNCR is once again grappling with severe a...  \n",
      "1  On January 15 the Delhi Directorate of Educati...  \n",
      "2  Human exposure to unhealthy levels of air poll...  \n",
      "3  Image source X American tech entrepreneur Brya...  \n",
      "4  Mumbai For a major part of November and Decemb...  \n"
     ]
    }
   ],
   "source": [
    "# Show the original data\n",
    "print(df[['title', 'content']].head())\n",
    "\n",
    "# Show the cleaned data\n",
    "print(df[['cleaned_title', 'cleaned_content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  grap stage iv restrictions reimposed in delhin...   \n",
      "1  delhi schools directed to shift to hybrid mode...   \n",
      "2  the hidden dangers of air pollution and fog on...   \n",
      "3  india would save more years of life by tech mi...   \n",
      "4       air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \n",
      "0  delhincr is once again grappling with severe a...  \n",
      "1  on january 15 the delhi directorate of educati...  \n",
      "2  human exposure to unhealthy levels of air poll...  \n",
      "3  image source x american tech entrepreneur brya...  \n",
      "4  mumbai for a major part of november and decemb...  \n"
     ]
    }
   ],
   "source": [
    "# Convert text to lowercase\n",
    "df['title'] = df['title'].str.lower()\n",
    "df['content'] = df['content'].str.lower()\n",
    "# Show the first few rows to verify the lowercase conversion\n",
    "print(df[['title', 'content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  grap stage iv restrictions reimposed in delhin...   \n",
      "1  delhi schools directed to shift to hybrid mode...   \n",
      "2  the hidden dangers of air pollution and fog on...   \n",
      "3  india would save more years of life by tech mi...   \n",
      "4       air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \n",
      "0  delhincr is once again grappling with severe a...  \n",
      "1  on january 15 the delhi directorate of educati...  \n",
      "2  human exposure to unhealthy levels of air poll...  \n",
      "3  image source x american tech entrepreneur brya...  \n",
      "4  mumbai for a major part of november and decemb...  \n"
     ]
    }
   ],
   "source": [
    "# Function to clean text (remove special characters, extra spaces)\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Check if the value is a string\n",
    "        return ''  # If not, return an empty string\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove special characters (keep only letters, digits, and spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Replace multiple consecutive whitespaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the title and content columns\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Show the cleaned title and content\n",
    "print(df[['title', 'content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  grap stage iv restrictions reimposed in delhin...   \n",
      "1  delhi schools directed to shift to hybrid mode...   \n",
      "2  the hidden dangers of air pollution and fog on...   \n",
      "3  india would save more years of life by tech mi...   \n",
      "4       air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \n",
      "0  delhincr is once again grappling with severe a...  \n",
      "1  on january 15 the delhi directorate of educati...  \n",
      "2  human exposure to unhealthy levels of air poll...  \n",
      "3  image source x american tech entrepreneur brya...  \n",
      "4  mumbai for a major part of november and decemb...  \n"
     ]
    }
   ],
   "source": [
    "# Function to remove URLs and email addresses\n",
    "def remove_links_and_emails(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply to the title and content columns\n",
    "df['title'] = df['title'].apply(remove_links_and_emails)\n",
    "df['content'] = df['content'].apply(remove_links_and_emails)\n",
    "\n",
    "# Show the result after removing links and emails\n",
    "print(df[['title', 'content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  grap stage iv restrictions reimposed in delhin...   \n",
      "1  delhi schools directed to shift to hybrid mode...   \n",
      "2  the hidden dangers of air pollution and fog on...   \n",
      "3  india would save more years of life by tech mi...   \n",
      "4       air pollution alarming at multiple locations   \n",
      "\n",
      "                                             content  \n",
      "0  delhincr is once again grappling with severe a...  \n",
      "1  on january 15 the delhi directorate of educati...  \n",
      "2  human exposure to unhealthy levels of air poll...  \n",
      "3  image source x american tech entrepreneur brya...  \n",
      "4  mumbai for a major part of november and decemb...  \n"
     ]
    }
   ],
   "source": [
    "from word2number import w2n\n",
    "\n",
    "# Function to convert numbers to words\n",
    "def convert_numbers_to_words(text):\n",
    "    # Ensure the input is a string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Replace all occurrences of numbers with their word equivalents\n",
    "    text = re.sub(r'\\d+', lambda x: str(w2n.word_to_num(x.group())), text)\n",
    "    return text\n",
    "\n",
    "# Apply to the title and content columns\n",
    "df['title'] = df['title'].apply(convert_numbers_to_words)\n",
    "df['content'] = df['content'].apply(convert_numbers_to_words)\n",
    "\n",
    "# Show the result after converting numbers to words\n",
    "print(df[['title', 'content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_tokens'] = df['title'].apply(tokenize_text)\n",
    "df['content_tokens'] = df['content'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        title_tokens  \\\n",
      "0  [grap, stage, iv, restrictions, reimposed, in,...   \n",
      "1  [delhi, schools, directed, to, shift, to, hybr...   \n",
      "2  [the, hidden, dangers, of, air, pollution, and...   \n",
      "3  [india, would, save, more, years, of, life, by...   \n",
      "4  [air, pollution, alarming, at, multiple, locat...   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [delhincr, is, once, again, grappling, with, s...  \n",
      "1  [on, january, 15, the, delhi, directorate, of,...  \n",
      "2  [human, exposure, to, unhealthy, levels, of, a...  \n",
      "3  [image, source, x, american, tech, entrepreneu...  \n",
      "4  [mumbai, for, a, major, part, of, november, an...  \n"
     ]
    }
   ],
   "source": [
    "print(df[['title_tokens', 'content_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        title_tokens  \\\n",
      "0  [grap, stage, iv, restrictions, reimposed, del...   \n",
      "1  [delhi, schools, directed, shift, hybrid, mode...   \n",
      "2  [hidden, dangers, air, pollution, fog, human, ...   \n",
      "3  [india, would, save, years, life, tech, millio...   \n",
      "4    [air, pollution, alarming, multiple, locations]   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [delhincr, grappling, severe, air, pollution, ...  \n",
      "1  [january, 15, delhi, directorate, education, d...  \n",
      "2  [human, exposure, unhealthy, levels, air, poll...  \n",
      "3  [image, source, x, american, tech, entrepreneu...  \n",
      "4  [mumbai, major, part, november, december, 2024...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already done\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords from tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Apply to the tokenized title and content columns\n",
    "df['title_tokens'] = df['title_tokens'].apply(remove_stopwords)\n",
    "df['content_tokens'] = df['content_tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Show the result after removing stopwords\n",
    "print(df[['title_tokens', 'content_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        title_tokens  \\\n",
      "0  [grap, stage, iv, restrict, reimpo, delhincr, ...   \n",
      "1  [delhi, school, direct, shift, hybrid, mode, c...   \n",
      "2  [hidden, danger, air, pollut, fog, human, health]   \n",
      "3  [india, would, save, year, life, tech, million...   \n",
      "4               [air, pollut, alarm, multipl, locat]   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [delhincr, grappl, sever, air, pollut, centr, ...  \n",
      "1  [januari, 15, delhi, director, educ, doe, dire...  \n",
      "2  [human, exposur, unhealthi, level, air, pollut...  \n",
      "3  [imag, sourc, x, american, tech, entrepreneur,...  \n",
      "4  [mumbai, major, part, novemb, decemb, 2024, po...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to apply stemming\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Apply stemming to the title and content tokens\n",
    "df['title_tokens'] = df['title_tokens'].apply(stem_tokens)\n",
    "df['content_tokens'] = df['content_tokens'].apply(stem_tokens)\n",
    "\n",
    "# Show the result after stemming\n",
    "print(df[['title_tokens', 'content_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        title_tokens  \\\n",
      "0  [grap, stage, iv, restrict, reimpo, delhincr, ...   \n",
      "1  [delhi, school, direct, shift, hybrid, mode, c...   \n",
      "2  [hidden, danger, air, pollut, fog, human, health]   \n",
      "3  [india, would, save, year, life, tech, million...   \n",
      "4               [air, pollut, alarm, multipl, locat]   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [delhincr, grappl, sever, air, pollut, centr, ...  \n",
      "1  [januari, 15, delhi, director, educ, doe, dire...  \n",
      "2  [human, exposur, unhealthi, level, air, pollut...  \n",
      "3  [imag, sourc, x, american, tech, entrepreneur,...  \n",
      "4  [mumbai, major, part, novemb, decemb, 2024, po...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization to the title and content tokens\n",
    "df['title_tokens'] = df['title_tokens'].apply(lemmatize_tokens)\n",
    "df['content_tokens'] = df['content_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Show the result after lemmatization\n",
    "print(df[['title_tokens', 'content_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        title_tokens  \\\n",
      "0  [grap, stage, iv, restrict, reimpo, delhincr, ...   \n",
      "1  [delhi, school, direct, shift, hybrid, mode, c...   \n",
      "2  [hidden, danger, air, pollut, fog, human, health]   \n",
      "3  [india, would, save, year, life, tech, million...   \n",
      "4               [air, pollut, alarm, multipl, locat]   \n",
      "\n",
      "                                      content_tokens  \n",
      "0  [delhincr, grappl, sever, air, pollut, centr, ...  \n",
      "1  [januari, 15, delhi, director, educ, doe, dire...  \n",
      "2  [human, exposur, unhealthi, level, air, pollut...  \n",
      "3  [imag, sourc, x, american, tech, entrepreneur,...  \n",
      "4  [mumbai, major, part, novemb, decemb, 2024, po...  \n"
     ]
    }
   ],
   "source": [
    "# Function to handle negations (retain words like 'not' for sentiment analysis)\n",
    "def handle_negations(tokens):\n",
    "    negation_words = {\"not\", \"no\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"neither\", \"nor\", \"don't\", \"isn't\", \"wasn't\", \"weren't\"}\n",
    "    new_tokens = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word in negation_words:\n",
    "            new_tokens.append(word)  # Retain negation words\n",
    "        elif i > 0 and tokens[i-1] in negation_words:\n",
    "            new_tokens.append(f\"not_{word}\")  # Attach \"not_\" to the next word\n",
    "        else:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens\n",
    "\n",
    "# Apply to the title and content tokens\n",
    "df['title_tokens'] = df['title_tokens'].apply(handle_negations)\n",
    "df['content_tokens'] = df['content_tokens'].apply(handle_negations)\n",
    "\n",
    "# Show the result after handling negations\n",
    "print(df[['title_tokens', 'content_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       cleaned_title  \\\n",
      "0  grap stage iv restrict reimpo delhincr amid ri...   \n",
      "1  delhi school direct shift hybrid mode class gr...   \n",
      "2          hidden danger air pollut fog human health   \n",
      "3  india would save year life tech millionair bry...   \n",
      "4                     air pollut alarm multipl locat   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  delhincr grappl sever air pollut centr reimpo ...  \n",
      "1  januari 15 delhi director educ doe direct scho...  \n",
      "2  human exposur unhealthi level air pollut often...  \n",
      "3  imag sourc x american tech entrepreneur bryan ...  \n",
      "4  mumbai major part novemb decemb 2024 pollut hi...  \n"
     ]
    }
   ],
   "source": [
    "# Reconstruct the cleaned title and content\n",
    "df['cleaned_title'] = df['title_tokens'].apply(lambda x: ' '.join(x))\n",
    "df['cleaned_content'] = df['content_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Show the cleaned title and content\n",
    "print(df[['cleaned_title', 'cleaned_content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"not\" in stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your preprocessed DataFrame\n",
    "preprocessed_file_path = 'preprocessed_data.csv'\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(preprocessed_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to {preprocessed_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
